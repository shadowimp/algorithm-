{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from line_profiler import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "def do_stuff(numbers):\n",
    "    s = sum(numbers)\n",
    "    l = [numbers[i]/43 for i in range(len(numbers))]\n",
    "    m = ['hello'+str(numbers[i]) for i in range(len(numbers))]\n",
    "\n",
    "numbers = [random.randint(1,100) for i in range(1000)]\n",
    "do_stuff(numbers)\n",
    "lp = LineProfiler()\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.001129 s\n",
      "File: <ipython-input-6-6fae3c1c07a7>\n",
      "Function: do_stuff at line 5\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     5                                               def do_stuff(self,numbers):\n",
      "     6         1         11.0     11.0      1.0          s = sum(numbers)\n",
      "     7         1        380.0    380.0     33.7          l = [numbers[i]/43 for i in range(len(numbers))]\n",
      "     8         1        738.0    738.0     65.4          m = ['hello'+str(numbers[i]) for i in range(len(numbers))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "import random\n",
    "\n",
    "class Solution:\n",
    "    def do_stuff(self,numbers):\n",
    "        s = sum(numbers)\n",
    "        l = [numbers[i]/43 for i in range(len(numbers))]\n",
    "        m = ['hello'+str(numbers[i]) for i in range(len(numbers))]\n",
    "\n",
    "numbers = [random.randint(1,100) for i in range(1000)]\n",
    "\n",
    "s = Solution()\n",
    "lp = LineProfiler()\n",
    "lp_wrapper = lp(s.do_stuff)\n",
    "lp_wrapper(numbers)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClusterBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_output, v ,T, k_cluster):\n",
    "        super(ClusterBlock, self).__init__( )\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.v = v  # number of node\n",
    "        self.T = T\n",
    "        self.k_cluster = k_cluster\n",
    "        self.linear1 = nn.Linear(n_input , 1)  # Dense\n",
    "        self.linear2 = nn.Linear(T ,k_cluster ) # clustering\n",
    "        self.softmax = nn.Softmax(dim =-1)\n",
    "        # support = 3 , each support has k GAT\n",
    "        self.attentions1 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.attentions2 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.attentions3 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, graph_list):  # x.shape = (b,v,T,f)\n",
    "        b, *_ = x.size()\n",
    "        # squeeze f\n",
    "        out = self.linear1(x)    #  out.shape = (b , v ,T,1)\n",
    "        out = out.view(-1, self.v, self.T)  #  out.shape = (b , v ,T)\n",
    "\n",
    "        # clustering\n",
    "        soft_cluster = self.softmax(self.linear2(out))   # out.shape = (b,v,k)\n",
    "        \n",
    "        # soft to hard        \n",
    "#         m = torch.transpose(torch.max(soft_cluster, -1)[0].repeat(2,1),0,1)\n",
    "        max_mat = torch.max(soft_cluster, -1)[0].unsqueeze(-1).repeat(1, 1, self.k_cluster)\n",
    "        one_mat = torch.ones(b, self.v, self.k_cluster)\n",
    "        zero_mat = torch.zeros(b, self.v, self.k_cluster)\n",
    "        hard_cluster = torch.where(soft_cluster-max_mat>=0, one_mat, zero_mat)\n",
    "        \n",
    "        #graph attention\n",
    "        gout = 0\n",
    "        for graph in graph_list:   # graph_list : (3,3 ,n , n )\n",
    "            graph1, graph2, graph3 = graph[0], graph[1], graph[2]\n",
    "#             cluster_mask_out = torch.zeros(3, b, self.v, self.T*self.output)\n",
    "            for i in range(self.k_cluster):\n",
    "                out1 = torch.mul(self.attentions1[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out2 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out3 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out = F.relu((out1 + out2  + out3)/3 )\n",
    "            gout += out\n",
    "#                 cluster_mask_out[0] += out1\n",
    "#                 cluster_mask_out[1] += out2\n",
    "#                 cluster_mask_out[2] += out3\n",
    "#             gout.append(torch.sum(cluster_mask_out, dim=0))\n",
    "\n",
    "        return gout/3 , hard_cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T , in_features, out_features, concat=False):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.concat = concat\n",
    "        self.W = nn.Parameter(torch.zeros(size=(T*in_features, T*out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*T*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, input, adj): # input.shape = ( b,v, t ,f1)\n",
    "        B , N , T = input.size()[0], input.size()[1], input.size()[2]     \n",
    "        \n",
    "        # h = xw , (b,N,t*f1)  * ( t*f1 ,  t*f2) -> (b ,N , t*f2)\n",
    "        input = input.view(B*N, -1)\n",
    "        h = torch.matmul(input, self.W).view(B , N , -1)\n",
    "        \n",
    "        # h.repeat(1,1,N) : (b, N, N*t*f2)  \n",
    "        # h.repeat(1,N,1) : (b ,N*N , t*f2)  \n",
    "        # output : (b , N , N, 2*t*f2)\n",
    "        a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=1).view(B, N, N, 2*T*self.out_features)\n",
    "        # e = a_input* a ,  (b,N,N,2*t*f2) * ( 2*t*f2 ,1) -> ( b ,N ,N )\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        # adj : (n, n) -> ( b , n ,n )\n",
    "        b_adj = adj.unsqueeze(0).repeat(B, 1, 1)\n",
    "        \n",
    "        attention = torch.where(b_adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "#         attention = self.dropout(attention, self.dropout, training=self.training)\n",
    "        \n",
    "        # (b,n,n) * (b,n ,t*f2)  -> (b, n , t*f2)\n",
    "        h_prime = torch.bmm(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = torch.rand(10 , 20 , 48 ,1)  # b,v,t, f\n",
    "test_label = torch.rand(10, 20 , 48 ,1)\n",
    "graph = torch.rand(20,20)\n",
    "graph_list = torch.rand(3,3,20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 48\n",
    "in_features = 1 \n",
    "out_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0434, -1.1207,  0.2787,  ..., -0.3739, -0.7780,  0.9089],\n",
       "         [ 0.1045, -1.2020,  0.2528,  ..., -0.3410, -0.7588,  0.8207],\n",
       "         [ 0.0497, -1.1719,  0.2624,  ..., -0.3532, -0.7659,  0.8534],\n",
       "         ...,\n",
       "         [-0.0233, -1.1288,  0.2170,  ..., -0.4523, -0.6655,  0.7165],\n",
       "         [-0.0233, -1.1288,  0.2170,  ..., -0.4523, -0.6655,  0.7165],\n",
       "         [-0.0233, -1.1288,  0.2170,  ..., -0.4523, -0.6655,  0.7165]],\n",
       "\n",
       "        [[-0.0661, -1.1882,  0.1886,  ..., -0.2305, -0.8488,  0.8532],\n",
       "         [-0.0656, -1.1889,  0.1867,  ..., -0.2228, -0.8516,  0.8696],\n",
       "         [-0.0672, -1.1864,  0.1933,  ..., -0.2486, -0.8420,  0.8148],\n",
       "         ...,\n",
       "         [ 0.0019, -1.0853,  0.2172,  ..., -0.2019, -0.8863,  0.9481],\n",
       "         [ 0.0019, -1.0853,  0.2172,  ..., -0.2019, -0.8863,  0.9481],\n",
       "         [ 0.0019, -1.0853,  0.2172,  ..., -0.2019, -0.8863,  0.9481]],\n",
       "\n",
       "        [[-0.0337, -1.2022,  0.4296,  ..., -0.2032, -0.9635,  1.0024],\n",
       "         [-0.0284, -1.2067,  0.4424,  ..., -0.1998, -0.9755,  1.0142],\n",
       "         [-0.0750, -1.1676,  0.3303,  ..., -0.2297, -0.8706,  0.9106],\n",
       "         ...,\n",
       "         [-0.0041, -1.1159,  0.3826,  ..., -0.0817, -1.0335,  0.9747],\n",
       "         [-0.0041, -1.1159,  0.3826,  ..., -0.0817, -1.0335,  0.9747],\n",
       "         [-0.0041, -1.1159,  0.3826,  ..., -0.0817, -1.0335,  0.9747]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1589, -1.0512,  0.2809,  ..., -0.1522, -0.7855,  0.9323],\n",
       "         [-0.1589, -1.0511,  0.2808,  ..., -0.1522, -0.7856,  0.9321],\n",
       "         [-0.1588, -1.0488,  0.2793,  ..., -0.1519, -0.7880,  0.9283],\n",
       "         ...,\n",
       "         [-0.0894, -1.0075,  0.3446,  ..., -0.1339, -0.7694,  0.8825],\n",
       "         [-0.0894, -1.0075,  0.3446,  ..., -0.1339, -0.7694,  0.8825],\n",
       "         [-0.0894, -1.0075,  0.3446,  ..., -0.1339, -0.7694,  0.8825]],\n",
       "\n",
       "        [[-0.0262, -1.0792,  0.2908,  ..., -0.2049, -0.8173,  0.8540],\n",
       "         [-0.0273, -1.0792,  0.2905,  ..., -0.1989, -0.8150,  0.8457],\n",
       "         [-0.0194, -1.0788,  0.2925,  ..., -0.2427, -0.8321,  0.9061],\n",
       "         ...,\n",
       "         [-0.0504, -1.0864,  0.3023,  ..., -0.2294, -0.8892,  0.8709],\n",
       "         [-0.0504, -1.0864,  0.3023,  ..., -0.2294, -0.8892,  0.8709],\n",
       "         [-0.0504, -1.0864,  0.3023,  ..., -0.2294, -0.8892,  0.8709]],\n",
       "\n",
       "        [[-0.2171, -1.2127,  0.3974,  ..., -0.1733, -0.6769,  1.0697],\n",
       "         [-0.2876, -1.1160,  0.1787,  ..., -0.3065, -0.8004,  0.9684],\n",
       "         [-0.2460, -1.1730,  0.3078,  ..., -0.2279, -0.7276,  1.0281],\n",
       "         ...,\n",
       "         [-0.2245, -1.1809,  0.3365,  ..., -0.1848, -0.6630,  0.9848],\n",
       "         [-0.2245, -1.1809,  0.3365,  ..., -0.1848, -0.6630,  0.9848],\n",
       "         [-0.2245, -1.1809,  0.3365,  ..., -0.1848, -0.6630,  0.9848]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layer = GraphAttentionLayer(T , in_features, out_features)\n",
    "model_layer(test_data,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.009483 s\n",
      "File: <ipython-input-8-20f176e5c274>\n",
      "Function: forward at line 20\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    20                                               def forward(self, input, adj): # input.shape = ( b,v, t ,f1)\n",
      "    21         1          8.0      8.0      0.1          B , N , T = input.size()[0], input.size()[1], input.size()[2]     \n",
      "    22                                                   \n",
      "    23                                                   # h = xw , (b,N,t*f1)  * ( t*f1 ,  t*f2) -> (b ,N , t*f2)\n",
      "    24         1         17.0     17.0      0.2          input = input.view(B*N, -1)\n",
      "    25         1       2774.0   2774.0     29.3          h = torch.matmul(input, self.W).view(B , N , -1)\n",
      "    26                                                   \n",
      "    27                                                   # h.repeat(1,1,N) : (b, N, N*t*f2)  \n",
      "    28                                                   # h.repeat(1,N,1) : (b ,N*N , t*f2)  \n",
      "    29                                                   # output : (b , N , N, 2*t*f2)\n",
      "    30         1       2944.0   2944.0     31.0          a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=1).view(B, N, N, 2*T*self.out_features)\n",
      "    31                                                   # e = a_input* a ,  (b,N,N,2*t*f2) * ( 2*t*f2 ,1) -> ( b ,N ,N )\n",
      "    32         1       3160.0   3160.0     33.3          e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n",
      "    33         1         77.0     77.0      0.8          zero_vec = -9e15*torch.ones_like(e)\n",
      "    34                                                   # adj : (n, n) -> ( b , n ,n )\n",
      "    35         1         55.0     55.0      0.6          b_adj = adj.unsqueeze(0).repeat(B, 1, 1)\n",
      "    36                                                   \n",
      "    37         1         84.0     84.0      0.9          attention = torch.where(b_adj > 0, e, zero_vec)\n",
      "    38         1         91.0     91.0      1.0          attention = F.softmax(attention, dim=-1)\n",
      "    39                                           #         attention = self.dropout(attention, self.dropout, training=self.training)\n",
      "    40                                                   \n",
      "    41                                                   # (b,n,n) * (b,n ,t*f2)  -> (b, n , t*f2)\n",
      "    42         1        269.0    269.0      2.8          h_prime = torch.bmm(attention, h)\n",
      "    43                                           \n",
      "    44         1          3.0      3.0      0.0          if self.concat:\n",
      "    45                                                       return F.elu(h_prime)\n",
      "    46                                                   else:\n",
      "    47         1          1.0      1.0      0.0              return h_prime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "model_layer = GraphAttentionLayer(T , in_features, out_features)\n",
    "# model_layer(test_data,graph)\n",
    "\n",
    "\n",
    "lp = LineProfiler()\n",
    "lp_wrapper = lp(model_layer.forward)\n",
    "lp_wrapper(test_data,graph)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T  = 48\n",
    "n_input = 1 \n",
    "n_output = 1 \n",
    "v = 20 \n",
    "k_cluster = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ClusterBlock(n_input, n_output, v ,T, k_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.003513 s\n",
      "File: <ipython-input-111-73d9242b4732>\n",
      "Function: forward at line 19\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "lp_wrapper = lp(model.forward)\n",
    "lp_wrapper(test_data,graph_list)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0057, 0.0000, 0.7051,  ..., 0.0000, 0.3279, 0.1443],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.7105,  ..., 0.0000, 0.3549, 0.1563],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2179, 0.0000, 0.6480,  ..., 0.0000, 0.2186, 0.1773],\n",
       "         ...,\n",
       "         [0.2328, 0.0000, 0.6930,  ..., 0.0000, 0.2687, 0.1587],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1547, 0.0000, 0.8107,  ..., 0.0000, 0.3011, 0.0470],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1702, 0.0000, 0.8093,  ..., 0.0000, 0.3134, 0.0377],\n",
       "         [0.1702, 0.0000, 0.8093,  ..., 0.0000, 0.3134, 0.0377],\n",
       "         [0.1702, 0.0000, 0.8093,  ..., 0.0000, 0.3134, 0.0377]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2562, 0.0000, 0.8257,  ..., 0.0000, 0.2968, 0.0629]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0747, 0.0000, 0.8517,  ..., 0.0000, 0.2523, 0.0682],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1716, 0.0000, 0.8160,  ..., 0.0000, 0.1519, 0.0423],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_data, graph_list)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
