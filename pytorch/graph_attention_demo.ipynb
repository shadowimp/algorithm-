{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_profiler import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "def do_stuff(numbers):\n",
    "    s = sum(numbers)\n",
    "    l = [numbers[i]/43 for i in range(len(numbers))]\n",
    "    m = ['hello'+str(numbers[i]) for i in range(len(numbers))]\n",
    "\n",
    "numbers = [random.randint(1,100) for i in range(1000)]\n",
    "do_stuff(numbers)\n",
    "lp = LineProfiler()\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_profiler import LineProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.001459 s\n",
      "File: <ipython-input-52-530b22a44142>\n",
      "Function: do_stuff at line 5\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     5                                               def do_stuff(self,numbers):\n",
      "     6         1         13.0     13.0      0.9          s = sum(numbers)\n",
      "     7         1        398.0    398.0     27.3          l = [numbers[i]/43 for i in range(len(numbers))]\n",
      "     8         1       1048.0   1048.0     71.8          m = ['hello'+str(numbers[i]) for i in range(len(numbers))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "import random\n",
    "\n",
    "class Solution:\n",
    "    def do_stuff(self,numbers):\n",
    "        s = sum(numbers)\n",
    "        l = [numbers[i]/43 for i in range(len(numbers))]\n",
    "        m = ['hello'+str(numbers[i]) for i in range(len(numbers))]\n",
    "\n",
    "numbers = [random.randint(1,100) for i in range(1000)]\n",
    "\n",
    "s = Solution()\n",
    "lp = LineProfiler()\n",
    "lp_wrapper = lp(s.do_stuff)\n",
    "lp_wrapper(numbers)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_output, v ,T, k_cluster):\n",
    "        super(ClusterBlock, self).__init__( )\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.v = v  # number of node\n",
    "        self.T = T\n",
    "        self.k_cluster = k_cluster\n",
    "        self.linear1 = nn.Linear(n_input , 1)  # Dense\n",
    "        self.linear2 = nn.Linear(T ,k_cluster ) # clustering\n",
    "        self.softmax = nn.Softmax(dim =-1)\n",
    "        # support = 3 , each support has k GAT\n",
    "        self.attentions1 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.attentions2 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.attentions3 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, graph_list):  # x.shape = (b,v,T,f)\n",
    "        b, *_ = x.size()\n",
    "        # squeeze f\n",
    "        out = self.linear1(x)    #  out.shape = (b , v ,T,1)\n",
    "        out = out.view(-1, self.v, self.T)  #  out.shape = (b , v ,T)\n",
    "\n",
    "        # clustering\n",
    "        soft_cluster = self.softmax(self.linear2(out))   # out.shape = (b,v,k)\n",
    "        \n",
    "        # soft to hard        \n",
    "#         m = torch.transpose(torch.max(soft_cluster, -1)[0].repeat(2,1),0,1)\n",
    "        max_mat = torch.max(soft_cluster, -1)[0].unsqueeze(-1).repeat(1, 1, self.k_cluster)\n",
    "        one_mat = torch.ones(b, self.v, self.k_cluster)\n",
    "        zero_mat = torch.zeros(b, self.v, self.k_cluster)\n",
    "        hard_cluster = torch.where(soft_cluster-max_mat>=0, one_mat, zero_mat)\n",
    "        \n",
    "        #graph attention\n",
    "        gout = 0\n",
    "        for graph in graph_list:   # graph_list : (3,3 ,n , n )\n",
    "            graph1, graph2, graph3 = graph[0], graph[1], graph[2]\n",
    "#             cluster_mask_out = torch.zeros(3, b, self.v, self.T*self.output)\n",
    "            for i in range(self.k_cluster):\n",
    "                out1 = torch.mul(self.attentions1[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out2 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out3 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out = F.relu((out1 + out2  + out3)/3 )\n",
    "            gout += out\n",
    "#                 cluster_mask_out[0] += out1\n",
    "#                 cluster_mask_out[1] += out2\n",
    "#                 cluster_mask_out[2] += out3\n",
    "#             gout.append(torch.sum(cluster_mask_out, dim=0))\n",
    "\n",
    "        return gout/3 , hard_cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T , in_features, out_features, concat=False):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.concat = concat\n",
    "        self.W = nn.Parameter(torch.zeros(size=(T*in_features, T*out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*T*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, input, adj): # input.shape = ( b,v, t ,f1)\n",
    "        B , N , T = input.size()[0], input.size()[1], input.size()[2]     \n",
    "        \n",
    "        # h = xw , (b,N,t*f1)  * ( t*f1 ,  t*f2) -> (b ,N , t*f2)\n",
    "        input = input.view(B*N, -1)\n",
    "        h = torch.matmul(input, self.W).view(B , N , -1)\n",
    "        \n",
    "        # h.repeat(1,1,N) : (b, N, N*t*f2)  \n",
    "        # h.repeat(1,N,1) : (b ,N*N , t*f2)  \n",
    "        # output : (b , N , N, 2*t*f2)\n",
    "        a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=1).view(B, N, N, 2*T*self.out_features)\n",
    "        # e = a_input* a ,  (b,N,N,2*t*f2) * ( 2*t*f2 ,1) -> ( b ,N ,N )\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        # adj : (n, n) -> ( b , n ,n )\n",
    "        b_adj = adj.unsqueeze(0).repeat(B, 1, 1)\n",
    "        \n",
    "        attention = torch.where(b_adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "#         attention = self.dropout(attention, self.dropout, training=self.training)\n",
    "        \n",
    "        # (b,n,n) * (b,n ,t*f2)  -> (b, n , t*f2)\n",
    "        h_prime = torch.bmm(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.rand(10 , 20 , 48 ,1)  # b,v,t, f\n",
    "test_label = torch.rand(10, 20 , 48 ,1)\n",
    "graph = torch.rand(20,20)\n",
    "graph_list = torch.rand(3,3,20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 48\n",
    "in_features = 1 \n",
    "out_features = 1\n",
    "v =20 \n",
    "k_cluster = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3428, -0.1388,  0.6871,  ..., -0.3074,  0.8060,  1.0066],\n",
       "         [ 0.3442, -0.1493,  0.6831,  ..., -0.3246,  0.8110,  0.9747],\n",
       "         [ 0.3481, -0.1800,  0.6712,  ..., -0.3753,  0.8257,  0.8807],\n",
       "         ...,\n",
       "         [ 0.3366, -0.1324,  0.6814,  ..., -0.3129,  0.8261,  0.9882],\n",
       "         [ 0.3366, -0.1324,  0.6814,  ..., -0.3129,  0.8261,  0.9882],\n",
       "         [ 0.3366, -0.1324,  0.6814,  ..., -0.3129,  0.8261,  0.9882]],\n",
       "\n",
       "        [[ 0.3266, -0.1457,  0.5593,  ..., -0.2689,  0.7146,  1.2123],\n",
       "         [ 0.3262, -0.1453,  0.5594,  ..., -0.2694,  0.7149,  1.2123],\n",
       "         [ 0.3263, -0.1454,  0.5594,  ..., -0.2692,  0.7148,  1.2123],\n",
       "         ...,\n",
       "         [ 0.4050, -0.1546,  0.5467,  ..., -0.2371,  0.7269,  1.2381],\n",
       "         [ 0.4050, -0.1546,  0.5467,  ..., -0.2371,  0.7269,  1.2381],\n",
       "         [ 0.4050, -0.1546,  0.5467,  ..., -0.2371,  0.7269,  1.2381]],\n",
       "\n",
       "        [[ 0.5069, -0.1338,  0.5035,  ..., -0.2158,  0.7635,  1.1479],\n",
       "         [ 0.5050, -0.1323,  0.5043,  ..., -0.2156,  0.7629,  1.1496],\n",
       "         [ 0.5061, -0.1332,  0.5039,  ..., -0.2157,  0.7632,  1.1486],\n",
       "         ...,\n",
       "         [ 0.5343, -0.1275,  0.5067,  ..., -0.2151,  0.7259,  1.1707],\n",
       "         [ 0.5343, -0.1275,  0.5067,  ..., -0.2151,  0.7259,  1.1707],\n",
       "         [ 0.5343, -0.1275,  0.5067,  ..., -0.2151,  0.7259,  1.1707]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4885, -0.1197,  0.5659,  ..., -0.3268,  0.9214,  1.1772],\n",
       "         [ 0.4767, -0.1157,  0.5768,  ..., -0.3255,  0.9172,  1.1606],\n",
       "         [ 0.4974, -0.1226,  0.5577,  ..., -0.3277,  0.9245,  1.1896],\n",
       "         ...,\n",
       "         [ 0.4946, -0.1148,  0.5726,  ..., -0.3259,  0.9130,  1.1776],\n",
       "         [ 0.4946, -0.1148,  0.5726,  ..., -0.3259,  0.9130,  1.1776],\n",
       "         [ 0.4946, -0.1148,  0.5726,  ..., -0.3259,  0.9130,  1.1776]],\n",
       "\n",
       "        [[ 0.4090, -0.1984,  0.7058,  ..., -0.1978,  1.0002,  1.1837],\n",
       "         [ 0.3576, -0.1902,  0.6740,  ..., -0.2644,  1.0284,  1.1672],\n",
       "         [ 0.4084, -0.1983,  0.7054,  ..., -0.1986,  1.0006,  1.1835],\n",
       "         ...,\n",
       "         [ 0.3607, -0.2301,  0.7234,  ..., -0.1053,  0.9544,  1.2639],\n",
       "         [ 0.3607, -0.2301,  0.7234,  ..., -0.1053,  0.9544,  1.2639],\n",
       "         [ 0.3607, -0.2301,  0.7234,  ..., -0.1053,  0.9544,  1.2639]],\n",
       "\n",
       "        [[ 0.4729, -0.2086,  0.6221,  ..., -0.2816,  0.7853,  1.2861],\n",
       "         [ 0.4445, -0.2731,  0.6148,  ..., -0.3379,  0.7357,  1.1992],\n",
       "         [ 0.4132, -0.3443,  0.6066,  ..., -0.4001,  0.6811,  1.1033],\n",
       "         ...,\n",
       "         [ 0.4424, -0.2723,  0.5971,  ..., -0.3361,  0.7166,  1.1836],\n",
       "         [ 0.4424, -0.2723,  0.5971,  ..., -0.3361,  0.7166,  1.1836],\n",
       "         [ 0.4424, -0.2723,  0.5971,  ..., -0.3361,  0.7166,  1.1836]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layer = GraphAttentionLayer(T , in_features, out_features)\n",
    "model_layer(test_data,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.255357 s\n",
      "File: <ipython-input-53-f187f0627aa8>\n",
      "Function: forward at line 19\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    19                                               def forward(self, x, graph_list):  # x.shape = (b,v,T,f)\n",
      "    20         1         10.0     10.0      0.0          b, *_ = x.size()\n",
      "    21                                                   # squeeze f\n",
      "    22         1        416.0    416.0      0.2          out = self.linear1(x)    #  out.shape = (b , v ,T,1)\n",
      "    23         1         40.0     40.0      0.0          out = out.view(-1, self.v, self.T)  #  out.shape = (b , v ,T)\n",
      "    24                                           \n",
      "    25                                                   # clustering\n",
      "    26         1        467.0    467.0      0.2          soft_cluster = self.softmax(self.linear2(out))   # out.shape = (b,v,k)\n",
      "    27                                                   \n",
      "    28                                                   # soft to hard        \n",
      "    29                                           #         m = torch.transpose(torch.max(soft_cluster, -1)[0].repeat(2,1),0,1)\n",
      "    30         1        224.0    224.0      0.1          max_mat = torch.max(soft_cluster, -1)[0].unsqueeze(-1).repeat(1, 1, self.k_cluster)\n",
      "    31         1         28.0     28.0      0.0          one_mat = torch.ones(b, self.v, self.k_cluster)\n",
      "    32         1         17.0     17.0      0.0          zero_mat = torch.zeros(b, self.v, self.k_cluster)\n",
      "    33         1        185.0    185.0      0.1          hard_cluster = torch.where(soft_cluster-max_mat>=0, one_mat, zero_mat)\n",
      "    34                                                   \n",
      "    35                                                   #graph attention\n",
      "    36         1          8.0      8.0      0.0          gout = 0\n",
      "    37        21        349.0     16.6      0.1          for graph in graph_list:   # graph_list : (3,3 ,n , n )\n",
      "    38        20        271.0     13.6      0.1              graph1, graph2, graph3 = graph[0], graph[1], graph[2]\n",
      "    39                                           #             cluster_mask_out = torch.zeros(3, b, self.v, self.T*self.output)\n",
      "    40        60        216.0      3.6      0.1              for i in range(self.k_cluster):\n",
      "    41        40      83731.0   2093.3     32.8                  out1 = torch.mul(self.attentions1[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
      "    42        40      79658.0   1991.5     31.2                  out2 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
      "    43        40      82859.0   2071.5     32.4                  out3 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
      "    44        40       6436.0    160.9      2.5                  out = F.relu((out1 + out2  + out3)/3 )\n",
      "    45        20        421.0     21.1      0.2              gout += out\n",
      "    46                                           #                 cluster_mask_out[0] += out1\n",
      "    47                                           #                 cluster_mask_out[1] += out2\n",
      "    48                                           #                 cluster_mask_out[2] += out3\n",
      "    49                                           #             gout.append(torch.sum(cluster_mask_out, dim=0))\n",
      "    50                                           \n",
      "    51         1         21.0     21.0      0.0          return gout/3 , hard_cluster \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "model = ClusterBlock( 1, 1, v ,T, k_cluster)\n",
    "# model_layer(test_data,graph)\n",
    "\n",
    "model.train()\n",
    "lp = LineProfiler()\n",
    "lp_wrapper = lp(model.forward)\n",
    "y = lp_wrapper(test_data,graph)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<DivBackward0>),\n",
       " tensor([[[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8512, -0.2636,  0.3433,  ...,  0.5017, -1.4792,  0.8147],\n",
       "         [ 0.8689, -0.2645,  0.3484,  ...,  0.5139, -1.4695,  0.8050],\n",
       "         [ 0.7708, -0.2597,  0.3200,  ...,  0.4466, -1.5234,  0.8584],\n",
       "         ...,\n",
       "         [ 0.7595, -0.2751,  0.4654,  ...,  0.4289, -1.5504,  0.9308],\n",
       "         [ 0.7595, -0.2751,  0.4654,  ...,  0.4289, -1.5504,  0.9308],\n",
       "         [ 0.7595, -0.2751,  0.4654,  ...,  0.4289, -1.5504,  0.9308]],\n",
       "\n",
       "        [[ 0.7915, -0.0874,  0.5578,  ...,  0.6531, -1.6323,  0.9227],\n",
       "         [ 0.7218, -0.1052,  0.5227,  ...,  0.5743, -1.6041,  0.9126],\n",
       "         [ 0.6658, -0.1195,  0.4944,  ...,  0.5109, -1.5814,  0.9046],\n",
       "         ...,\n",
       "         [ 0.6678, -0.0461,  0.5616,  ...,  0.6656, -1.6160,  0.9089],\n",
       "         [ 0.6678, -0.0461,  0.5616,  ...,  0.6656, -1.6160,  0.9089],\n",
       "         [ 0.6678, -0.0461,  0.5616,  ...,  0.6656, -1.6160,  0.9089]],\n",
       "\n",
       "        [[ 0.7772, -0.0105,  0.3533,  ...,  0.5313, -1.4442,  0.7907],\n",
       "         [ 0.7157, -0.0390,  0.4004,  ...,  0.6033, -1.4209,  0.9102],\n",
       "         [ 0.6992, -0.0466,  0.4130,  ...,  0.6227, -1.4147,  0.9423],\n",
       "         ...,\n",
       "         [ 0.8386,  0.0017,  0.3846,  ...,  0.6408, -1.4428,  0.8547],\n",
       "         [ 0.8386,  0.0017,  0.3846,  ...,  0.6408, -1.4428,  0.8547],\n",
       "         [ 0.8386,  0.0017,  0.3846,  ...,  0.6408, -1.4428,  0.8547]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8492, -0.0115,  0.3972,  ...,  0.5750, -1.6932,  0.8564],\n",
       "         [ 0.7155, -0.0978,  0.4141,  ...,  0.5446, -1.6963,  0.8875],\n",
       "         [ 0.8119, -0.0356,  0.4019,  ...,  0.5665, -1.6941,  0.8651],\n",
       "         ...,\n",
       "         [ 0.9196,  0.0572,  0.3627,  ...,  0.6130, -1.6017,  0.7957],\n",
       "         [ 0.9196,  0.0572,  0.3627,  ...,  0.6130, -1.6017,  0.7957],\n",
       "         [ 0.9196,  0.0572,  0.3627,  ...,  0.6130, -1.6017,  0.7957]],\n",
       "\n",
       "        [[ 0.8955, -0.3323,  0.4491,  ...,  0.4528, -1.4554,  0.9449],\n",
       "         [ 0.9069, -0.3395,  0.4404,  ...,  0.4584, -1.4438,  0.9376],\n",
       "         [ 0.9113, -0.3424,  0.4370,  ...,  0.4606, -1.4393,  0.9348],\n",
       "         ...,\n",
       "         [ 0.8902, -0.3148,  0.4310,  ...,  0.4325, -1.4308,  1.0048],\n",
       "         [ 0.8902, -0.3148,  0.4310,  ...,  0.4325, -1.4308,  1.0048],\n",
       "         [ 0.8902, -0.3148,  0.4310,  ...,  0.4325, -1.4308,  1.0048]],\n",
       "\n",
       "        [[ 0.8029, -0.0269,  0.4394,  ...,  0.5633, -1.5531,  0.8163],\n",
       "         [ 0.8102, -0.0320,  0.4374,  ...,  0.5636, -1.5463,  0.8253],\n",
       "         [ 0.7923, -0.0195,  0.4424,  ...,  0.5627, -1.5631,  0.8031],\n",
       "         ...,\n",
       "         [ 0.9395, -0.0990,  0.3827,  ...,  0.6432, -1.4099,  0.8716],\n",
       "         [ 0.9395, -0.0990,  0.3827,  ...,  0.6432, -1.4099,  0.8716],\n",
       "         [ 0.9395, -0.0990,  0.3827,  ...,  0.6432, -1.4099,  0.8716]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layer(test_data,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "T  = 48\n",
    "n_input = 1 \n",
    "n_output = 1 \n",
    "v = 20 \n",
    "k_cluster = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ClusterBlock(n_input, n_output, v ,T, k_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.003513 s\n",
      "File: <ipython-input-111-73d9242b4732>\n",
      "Function: forward at line 19\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "lp_wrapper = lp(model.forward)\n",
    "lp_wrapper(test_data,graph_list)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0057, 0.0000, 0.7051,  ..., 0.0000, 0.3279, 0.1443],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.7105,  ..., 0.0000, 0.3549, 0.1563],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2179, 0.0000, 0.6480,  ..., 0.0000, 0.2186, 0.1773],\n",
       "         ...,\n",
       "         [0.2328, 0.0000, 0.6930,  ..., 0.0000, 0.2687, 0.1587],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1547, 0.0000, 0.8107,  ..., 0.0000, 0.3011, 0.0470],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1702, 0.0000, 0.8093,  ..., 0.0000, 0.3134, 0.0377],\n",
       "         [0.1702, 0.0000, 0.8093,  ..., 0.0000, 0.3134, 0.0377],\n",
       "         [0.1702, 0.0000, 0.8093,  ..., 0.0000, 0.3134, 0.0377]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2562, 0.0000, 0.8257,  ..., 0.0000, 0.2968, 0.0629]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0747, 0.0000, 0.8517,  ..., 0.0000, 0.2523, 0.0682],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1716, 0.0000, 0.8160,  ..., 0.0000, 0.1519, 0.0423],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_data, graph_list)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
