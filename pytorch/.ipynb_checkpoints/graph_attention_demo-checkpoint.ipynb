{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_output, v ,T, k_cluster):\n",
    "        super(ClusterBlock, self).__init__( )\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.v = v  # number of node\n",
    "        self.T = T\n",
    "        self.k_cluster = k_cluster\n",
    "        self.linear1 = nn.Linear(n_input , 1)  # Dense\n",
    "        self.linear2 = nn.Linear(T ,k_cluster ) # clustering\n",
    "        self.softmax = nn.Softmax(dim =-1)\n",
    "        # support = 3 , each support has k GAT\n",
    "        self.attentions1 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.attentions2 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.attentions3 = [GraphAttentionLayer(T, n_input, n_output) for _ in range(k_cluster)]\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x, graph_list):  # x.shape = (b,v,T,f)\n",
    "        b, *_ = x.size()\n",
    "        # squeeze f\n",
    "        out = self.linear1(x)    #  out.shape = (b , v ,T,1)\n",
    "        out = out.view(-1, self.v, self.T)  #  out.shape = (b , v ,T)\n",
    "\n",
    "        # clustering\n",
    "        soft_cluster = self.softmax(self.linear2(out))   # out.shape = (b,v,k)\n",
    "        \n",
    "        # soft to hard        \n",
    "#         m = torch.transpose(torch.max(soft_cluster, -1)[0].repeat(2,1),0,1)\n",
    "        max_mat = torch.max(soft_cluster, -1)[0].unsqueeze(-1).repeat(1, 1, self.k_cluster)\n",
    "        one_mat = torch.ones(b, self.v, self.k_cluster)\n",
    "        zero_mat = torch.zeros(b, self.v, self.k_cluster)\n",
    "        hard_cluster = torch.where(soft_cluster-max_mat>=0, one_mat, zero_mat)\n",
    "        \n",
    "        #graph attention\n",
    "        gout = 0\n",
    "        for graph in graph_list:   # graph_list : (3,3 ,n , n )\n",
    "            graph1, graph2, graph3 = graph[0], graph[1], graph[2]\n",
    "#             cluster_mask_out = torch.zeros(3, b, self.v, self.T*self.output)\n",
    "            for i in range(self.k_cluster):\n",
    "                out1 = torch.mul(self.attentions1[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out2 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out3 = torch.mul(self.attentions2[i](x, graph1), hard_cluster[:, :, i].unsqueeze(-1))\n",
    "                out = F.relu((out1 + out2  + out3)/3 )\n",
    "            gout += out\n",
    "#                 cluster_mask_out[0] += out1\n",
    "#                 cluster_mask_out[1] += out2\n",
    "#                 cluster_mask_out[2] += out3\n",
    "#             gout.append(torch.sum(cluster_mask_out, dim=0))\n",
    "\n",
    "        return gout/3 , hard_cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T , in_features, out_features, concat=False):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.concat = concat\n",
    "        self.W = nn.Parameter(torch.zeros(size=(T*in_features, T*out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*T*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, input, adj): # input.shape = ( b,v, t ,f1)\n",
    "        B , N , T = input.size()[0], input.size()[1], input.size()[2]     \n",
    "        \n",
    "        # h = xw , (b,N,t*f1)  * ( t*f1 ,  t*f2) -> (b ,N , t*f2)\n",
    "        input = input.view(B*N, -1)\n",
    "        h = torch.matmul(input, self.W).view(B , N , -1)\n",
    "        \n",
    "        # h.repeat(1,1,N) : (b, N, N*t*f2)  \n",
    "        # h.repeat(1,N,1) : (b ,N*N , t*f2)  \n",
    "        # output : (b , N , N, 2*t*f2)\n",
    "        a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=1).view(B, N, N, 2*T*self.out_features)\n",
    "        # e = a_input* a ,  (b,N,N,2*t*f2) * ( 2*t*f2 ,1) -> ( b ,N ,N )\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        # adj : (n, n) -> ( b , n ,n )\n",
    "        b_adj = adj.unsqueeze(0).repeat(B, 1, 1)\n",
    "        \n",
    "        attention = torch.where(b_adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "#         attention = self.dropout(attention, self.dropout, training=self.training)\n",
    "        \n",
    "        # (b,n,n) * (b,n ,t*f2)  -> (b, n , t*f2)\n",
    "        h_prime = torch.bmm(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.rand(10 , 20 , 48 ,1)  # b,v,t, f\n",
    "test_label = torch.rand(10, 20 , 48 ,1)\n",
    "graph = torch.rand(20,20)\n",
    "graph_list = torch.rand(3,3,20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1944,  0.5102, -0.3534,  ..., -0.5934, -0.3720, -1.7551],\n",
       "         [ 0.2738,  0.5211, -0.3906,  ..., -0.6415, -0.4070, -1.7689],\n",
       "         [ 0.2683,  0.5204, -0.3881,  ..., -0.6382, -0.4046, -1.7679],\n",
       "         ...,\n",
       "         [ 0.2265,  0.5083, -0.3236,  ..., -0.6340, -0.4281, -1.8062],\n",
       "         [ 0.2265,  0.5083, -0.3236,  ..., -0.6340, -0.4281, -1.8062],\n",
       "         [ 0.2265,  0.5083, -0.3236,  ..., -0.6340, -0.4281, -1.8062]],\n",
       "\n",
       "        [[ 0.1704,  0.6645, -0.6333,  ..., -0.5211, -0.6062, -1.8961],\n",
       "         [ 0.3105,  0.7658, -0.6013,  ..., -0.6110, -0.6656, -1.7635],\n",
       "         [ 0.1354,  0.6392, -0.6413,  ..., -0.4986, -0.5913, -1.9292],\n",
       "         ...,\n",
       "         [ 0.3333,  0.7678, -0.5493,  ..., -0.5518, -0.6182, -1.8569],\n",
       "         [ 0.3333,  0.7678, -0.5493,  ..., -0.5518, -0.6182, -1.8569],\n",
       "         [ 0.3333,  0.7678, -0.5493,  ..., -0.5518, -0.6182, -1.8569]],\n",
       "\n",
       "        [[ 0.2952,  0.5778, -0.4222,  ..., -0.6484, -0.5660, -1.7679],\n",
       "         [ 0.3128,  0.5388, -0.3102,  ..., -0.5990, -0.5876, -1.7525],\n",
       "         [ 0.2852,  0.5998, -0.4854,  ..., -0.6763, -0.5538, -1.7765],\n",
       "         ...,\n",
       "         [ 0.2470,  0.5941, -0.3847,  ..., -0.5423, -0.5548, -1.7200],\n",
       "         [ 0.2470,  0.5941, -0.3847,  ..., -0.5423, -0.5548, -1.7200],\n",
       "         [ 0.2470,  0.5941, -0.3847,  ..., -0.5423, -0.5548, -1.7200]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3660,  0.5521, -0.6317,  ..., -0.6874, -0.5712, -1.7303],\n",
       "         [ 0.3725,  0.5494, -0.6187,  ..., -0.6102, -0.5345, -1.7315],\n",
       "         [ 0.3732,  0.5491, -0.6173,  ..., -0.6020, -0.5305, -1.7316],\n",
       "         ...,\n",
       "         [ 0.3412,  0.5332, -0.5776,  ..., -0.5701, -0.4730, -1.6769],\n",
       "         [ 0.3412,  0.5332, -0.5776,  ..., -0.5701, -0.4730, -1.6769],\n",
       "         [ 0.3412,  0.5332, -0.5776,  ..., -0.5701, -0.4730, -1.6769]],\n",
       "\n",
       "        [[ 0.4993,  0.6390, -0.5890,  ..., -0.5480, -0.5727, -1.7809],\n",
       "         [ 0.4036,  0.6464, -0.7369,  ..., -0.5373, -0.5839, -1.7288],\n",
       "         [ 0.4089,  0.6460, -0.7287,  ..., -0.5379, -0.5833, -1.7317],\n",
       "         ...,\n",
       "         [ 0.4834,  0.6411, -0.6169,  ..., -0.5548, -0.5706, -1.7701],\n",
       "         [ 0.4834,  0.6411, -0.6169,  ..., -0.5548, -0.5706, -1.7701],\n",
       "         [ 0.4834,  0.6411, -0.6169,  ..., -0.5548, -0.5706, -1.7701]],\n",
       "\n",
       "        [[ 0.2545,  0.4434, -0.4745,  ..., -0.6546, -0.5353, -1.8012],\n",
       "         [ 0.1998,  0.4139, -0.4879,  ..., -0.6895, -0.5369, -1.7613],\n",
       "         [ 0.2055,  0.4170, -0.4865,  ..., -0.6859, -0.5368, -1.7654],\n",
       "         ...,\n",
       "         [ 0.2553,  0.4201, -0.4611,  ..., -0.6994, -0.5164, -1.7304],\n",
       "         [ 0.2553,  0.4201, -0.4611,  ..., -0.6994, -0.5164, -1.7304],\n",
       "         [ 0.2553,  0.4201, -0.4611,  ..., -0.6994, -0.5164, -1.7304]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 48\n",
    "in_features = 1 \n",
    "out_features = 1\n",
    "model_layer = GraphAttentionLayer(T , in_features, out_features)\n",
    "model_layer(test_data,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "T  = 48\n",
    "n_input = 1 \n",
    "n_output = 1 \n",
    "v = 20 \n",
    "k_cluster = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClusterBlock(n_input, n_output, v ,T, k_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 48])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_data, graph_list)[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
